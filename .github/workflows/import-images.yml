# Automated import + image optimization for etainkiely-com
# Trigger: manual (workflow_dispatch). Optional input: google_sites_url (comma-separated).
on:
  workflow_dispatch:
    inputs:
      google_sites_url:
        description: 'Comma-separated published Google Sites URLs (optional). If empty, provide google_sites_export.zip in repo root.'
        required: false
      include_at_repos:
        description: 'Comma-separated additional public repos to clone (default: ATU-Computational-Math/etainkiely-com)'
        required: false

jobs:
  import_images:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install tools
        run: |
          sudo apt-get update -y
          sudo apt-get install -y wget unzip imagemagick webp jq

      - name: Prepare workspace
        run: |
          mkdir -p work out assets/images/imported

      - name: Clone ATU repo (public)
        run: |
          git clone --depth 1 https://github.com/ATU-Computational-Math/etainkiely-com.git work/atu || echo "skip"

      - name: Mirror Game-Flo Pages (if public)
        run: |
          URL="https://etainkiely.github.io/Game-Flo-Projects/"
          wget --mirror --convert-links --adjust-extension --page-requisites --no-parent -P work/ghpages "$URL" || echo "Game-Flo mirror failed or not public"

      - name: Add provided Google Sites export (if present)
        run: |
          if [ -f google_sites_export.zip ]; then
            unzip -q google_sites_export.zip -d work/google_sites_export || true
          else
            echo "No google_sites_export.zip found"
          fi

      - name: Mirror provided Google Sites URLs (if input given)
        if: ${{ inputs.google_sites_url != '' }}
        run: |
          IFS=',' read -ra URLS <<< "${{ inputs.google_sites_url }}"
          for u in "${URLS[@]}"; do
            u=$(echo "$u" | xargs)
            wget --mirror --convert-links --adjust-extension --page-requisites --no-parent -P work/google_sites_mirror "$u" || echo "mirror failed for $u"
          done

      - name: Collect candidate images
        run: |
          mkdir -p out/candidates
          find work -type f \( -iname "*.png" -o -iname "*.jpg" -o -iname "*.jpeg" -o -iname "*.webp" -o -iname "*.gif" -o -iname "*.svg" \) -print0 \
            | xargs -0 -I{} bash -c 'cp -n "{}" "out/candidates/$(basename "{}")" || true'
          find . -maxdepth 3 -type f \( -iname "*.png" -o -iname "*.jpg" -o -iname "*.jpeg" -o -iname "*.webp" -o -iname "*.gif" -o -iname "*.svg" \) -print0 \
            | xargs -0 -I{} bash -c 'cp -n "{}" "out/candidates/$(basename "{}")" || true'
          ls -la out/candidates | sed -n '1,200p'

      - name: Prioritize candidates by filename keywords
        run: |
          mkdir -p out/selected
          cd out/candidates
          KEYWORDS="student|students|group|presentation|poster|celtic|knot|fern|fractal|diagram|screenshot|game|playtest|play-test"
          for f in *; do
            if echo "$f" | grep -Ei "$KEYWORDS" >/dev/null 2>&1; then
              cp -n "$f" ../selected/"$f"
            fi
          done
          cd ..
          if [ $(ls selected 2>/dev/null | wc -l || true) -lt 1 ]; then
            ls candidates | head -n 50 | xargs -I{} cp candidates/{} selected/{} || true
          fi
          ls -la selected | sed -n '1,200p'

      - name: Optimize selected images (create JPG + WebP at 480/1024/2048)
        run: |
          mkdir -p assets/images/imported
          python3 - <<'PY'
import os,subprocess,json
src='out/selected'
out='assets/images/imported'
if not os.path.isdir(src):
    src='out/candidates'
records=[]
for fn in sorted(os.listdir(src)):
    srcp=os.path.join(src,fn)
    base=os.path.splitext(fn)[0]
    safe=''.join(c if c.isalnum() or c in '-._' else '-' for c in base).lower()
    dest_prefix=os.path.join(out,safe)
    os.makedirs(os.path.dirname(dest_prefix),exist_ok=True)
    sizes=[480,1024,2048]
    size_files={}
    for s in sizes:
        jpg=f"{dest_prefix}-{s}.jpg"
        try:
            subprocess.run(['convert', srcp, '-strip', '-resize', f'{s}x', '-quality', '82', jpg], check=True)
            webp=f"{dest_prefix}-{s}.webp"
            subprocess.run(['cwebp','-q','82', jpg, '-o', webp], check=True)
            size_files[str(s)]={'jpg':jpg,'webp':webp}
        except Exception:
            pass
    records.append({'source':srcp,'slug':safe,'sizes':size_files})
with open('assets/images/imported/manifest.json','w') as f:
    json.dump({'generated_at':__import__('datetime').datetime.utcnow().isoformat()+'Z','count':len(records),'images':records},f,indent=2)
print("manifest written with",len(records))
PY
          ls -la assets/images/imported | sed -n '1,200p'

      - name: Create branch & open PR with imported images
        uses: peter-evans/create-pull-request@v5
        with:
          branch: images/import-from-scan-${{ github.run_id }}
          commit-message: "chore(images): import candidate images and manifest (automated scan)"
          title: "Import candidate images â€” automated scan"
          body: |
            This PR was created by an automated workflow to import candidate images (student/process/result, celtic motifs, posters, screenshots).
            Review the assets/images/imported/manifest.json for provenance and consent flags (default=false). Remove or flag any student images without consent before merging.
          labels: automated,image-import

      - name: Show manifest summary
        run: |
          jq '. | {generated_at: .generated_at, count: .count} + (.images[0:10] | {sample: .})' assets/images/imported/manifest.json || true
